{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available variables:\n",
      "- In\n",
      "- Out\n",
      "- exit\n",
      "- get_ipython\n",
      "- open\n",
      "- quit\n",
      "\n",
      "‚úó documents variable not found\n"
     ]
    }
   ],
   "source": [
    "# Check what variables exist in your notebook\n",
    "print(\"Available variables:\")\n",
    "for var_name in dir():\n",
    "    if not var_name.startswith('_'):\n",
    "        print(f\"- {var_name}\")\n",
    "\n",
    "# Check specifically for documents-related variables\n",
    "if 'documents' in locals():\n",
    "    print(f\"\\n‚úì documents variable exists with {len(documents)} items\")\n",
    "else:\n",
    "    print(\"\\n‚úó documents variable not found\")\n",
    "    \n",
    "if 'success_list' in locals():\n",
    "    print(f\"‚úì success_list exists with {len(success_list)} files\")\n",
    "    \n",
    "if 'failure_list' in locals():\n",
    "    print(f\"‚úì failure_list exists with {len(failure_list)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 55 0 (offset 0)\n",
      "Ignoring wrong pointing object 76 0 (offset 0)\n",
      "Ignoring wrong pointing object 78 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF documents...\n",
      "Found 7 PDF files\n",
      "[1/7] Loading: SJ_Developers.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 97 0 (offset 0)\n",
      "Ignoring wrong pointing object 99 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Loaded 9 pages\n",
      "[2/7] Loading: Sri Sathya Sai Serenity_Layout_Final.pdf\n",
      "  ‚úì Loaded 1 pages\n",
      "[3/7] Loading: AmariFarms_Brochure.pdf\n",
      "  ‚úì Loaded 30 pages\n",
      "[4/7] Loading: VILLA 01.pdf\n",
      "  ‚úì Loaded 2 pages\n",
      "[5/7] Loading: VILLA 02.pdf\n",
      "  ‚úì Loaded 2 pages\n",
      "[6/7] Loading: REGISTRATION OF FIRMS.pdf\n",
      "  ‚úì Loaded 1 pages\n",
      "[7/7] Loading: CIBW1022010.pdf\n",
      "  ‚úì Loaded 17 pages\n",
      "\n",
      "Total: 62 pages loaded\n",
      "\n",
      "‚úÖ SUCCESS: 62 pages ready for chunking\n",
      "Files loaded: 7\n",
      "  - SJ_Developers.pdf\n",
      "  - VILLA 01.pdf\n",
      "  - CIBW1022010.pdf\n",
      "  - AmariFarms_Brochure.pdf\n",
      "  - VILLA 02.pdf\n",
      "  - Sri Sathya Sai Serenity_Layout_Final.pdf\n",
      "  - REGISTRATION OF FIRMS.pdf\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "#from the documents loading all pdf files from the data/documents folder\n",
    "import os\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "def load_pdf_files():\n",
    "    \"\"\"\n",
    "    Load all PDF files from data/documents folder\n",
    "    \"\"\"\n",
    "    base_folder = Path('../data/documents')\n",
    "    all_documents = []\n",
    "    \n",
    "    # Find all PDF files\n",
    "    pdf_files = list(base_folder.rglob(\"*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files\")\n",
    "    \n",
    "    for i, pdf_file in enumerate(pdf_files, 1):\n",
    "        print(f\"[{i}/{len(pdf_files)}] Loading: {pdf_file.name}\")\n",
    "        \n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            docs = loader.load()\n",
    "            \n",
    "            # Add source file metadata\n",
    "            for doc in docs:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_path'] = str(pdf_file)\n",
    "            \n",
    "            all_documents.extend(docs)\n",
    "            print(f\"  ‚úì Loaded {len(docs)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Failed: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal: {len(all_documents)} pages loaded\")\n",
    "    return all_documents\n",
    "\n",
    "# Load documents\n",
    "print(\"Loading PDF documents...\")\n",
    "documents = load_pdf_files()\n",
    "\n",
    "# Verify loading\n",
    "if documents:\n",
    "    print(f\"\\n‚úÖ SUCCESS: {len(documents)} pages ready for chunking\")\n",
    "    \n",
    "    # Show files loaded\n",
    "    files_loaded = set(doc.metadata.get('source_file') for doc in documents)\n",
    "    print(f\"Files loaded: {len(files_loaded)}\")\n",
    "    for file in files_loaded:\n",
    "        print(f\"  - {file}\")\n",
    "else:\n",
    "    print(\"‚ùå No documents loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating chunks from 62 loaded pages...\n",
      "‚úì Successfully created 83 chunks\n",
      "\n",
      "üìä CHUNKS BY FILE:\n",
      "----------------------------------------\n",
      "  SJ_Developers.pdf: 10 chunks\n",
      "  AmariFarms_Brochure.pdf: 8 chunks\n",
      "  VILLA 01.pdf: 2 chunks\n",
      "  VILLA 02.pdf: 2 chunks\n",
      "  CIBW1022010.pdf: 61 chunks\n",
      "\n",
      "üìÑ SAMPLE CHUNK:\n",
      "--------------------------------------------------\n",
      "File: SJ_Developers.pdf\n",
      "Page: 0\n",
      "Length: 48 characters\n",
      "Content preview:\n",
      "SJ DevelopersAnantapuram, Andhra Pradesh., India...\n"
     ]
    }
   ],
   "source": [
    "# Use your already loaded documents to create chunks\n",
    "#making chunks from the loaded documents\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "  # Assuming you have a data_loader module to load your documents\n",
    "\n",
    "# Configure chunk settings\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "def create_chunks_from_documents(documents):\n",
    "    \"\"\"\n",
    "    Create chunks from your loaded PDF documents\n",
    "    \"\"\"\n",
    "    print(f\"Creating chunks from {len(documents)} loaded pages...\")\n",
    "    \n",
    "    # Initialize text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    print(f\"‚úì Successfully created {len(chunks)} chunks\")\n",
    "    \n",
    "    # Show chunks distribution by source file\n",
    "    chunk_stats = {}\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get('source_file', 'Unknown')\n",
    "        chunk_stats[source] = chunk_stats.get(source, 0) + 1\n",
    "    \n",
    "    print(\"\\nüìä CHUNKS BY FILE:\")\n",
    "    print(\"-\" * 40)\n",
    "    for file, count in chunk_stats.items():\n",
    "        print(f\"  {file}: {count} chunks\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Create chunks from your loaded documents\n",
    "chunks = create_chunks_from_documents(documents)\n",
    "\n",
    "# Show sample chunk\n",
    "if chunks:\n",
    "    print(f\"\\nüìÑ SAMPLE CHUNK:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"File: {chunks[0].metadata.get('source_file')}\")\n",
    "    print(f\"Page: {chunks[0].metadata.get('page', 'N/A')}\")\n",
    "    print(f\"Length: {len(chunks[0].page_content)} characters\")\n",
    "    print(f\"Content preview:\\n{chunks[0].page_content[:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (5.0.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from sentence-transformers) (4.53.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from sentence-transformers) (1.7.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from sentence-transformers) (0.33.0)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from sentence-transformers) (4.14.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.3)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.4.26)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/torch-env/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Setting up ChromaDB vector store...\n",
      "Loading embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x5/gxfv5zh57dd8qw9f49g4t1rh0000gn/T/ipykernel_9826/532510386.py:16: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/x5/gxfv5zh57dd8qw9f49g4t1rh0000gn/T/ipykernel_9826/532510386.py\", line 47, in <module>\n",
      "    vectordb = store_in_chromadb(chunks)\n",
      "  File \"/var/folders/x5/gxfv5zh57dd8qw9f49g4t1rh0000gn/T/ipykernel_9826/532510386.py\", line 16, in store_in_chromadb\n",
      "    embeddings = HuggingFaceEmbeddings(\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 222, in warn_if_direct_instance\n",
      "    return wrapped(self, *args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py\", line 84, in __init__\n",
      "    import sentence_transformers\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/sentence_transformers/__init__.py\", line 10, in <module>\n",
      "    from sentence_transformers.backend import (\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/sentence_transformers/backend.py\", line 11, in <module>\n",
      "    from sentence_transformers.util import disable_datasets_caching, is_datasets_available\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/sentence_transformers/util.py\", line 20, in <module>\n",
      "    import torch\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ChromaDB vector store...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Store chunks in ChromaDB\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunks\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m chunks:\n\u001b[0;32m---> 47\u001b[0m     vectordb \u001b[38;5;241m=\u001b[39m \u001b[43mstore_in_chromadb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ùå No chunks available for storage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m, in \u001b[0;36mstore_in_chromadb\u001b[0;34m(chunks)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Create ChromaDB vector store\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating ChromaDB vector store...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m vectordb \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvector_store_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msj_developers_knowledge\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     33\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Persist to disk\u001b[39;00m\n\u001b[1;32m     36\u001b[0m vectordb\u001b[38;5;241m.\u001b[39mpersist()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:887\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    886\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 887\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:843\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[1;32m    838\u001b[0m         api\u001b[38;5;241m=\u001b[39mchroma_collection\u001b[38;5;241m.\u001b[39m_client,\n\u001b[1;32m    839\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m    840\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[1;32m    841\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m    842\u001b[0m     ):\n\u001b[0;32m--> 843\u001b[0m         \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    849\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:277\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py:115\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    113\u001b[0m     sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer\u001b[38;5;241m.\u001b[39mstop_multi_process_pool(pool)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_kwargs\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1110\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m             all_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([emb\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m all_embeddings])\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1110\u001b[0m             all_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([emb\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m all_embeddings])\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(all_embeddings, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m   1112\u001b[0m     all_embeddings \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mfrom_numpy(embedding) \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m all_embeddings]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-env/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1110\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1108\u001b[0m             all_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([emb\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m all_embeddings])\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1110\u001b[0m             all_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([\u001b[43memb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m all_embeddings])\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(all_embeddings, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m   1112\u001b[0m     all_embeddings \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mfrom_numpy(embedding) \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m all_embeddings]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "#storing chunks in ChromaDB vector database\n",
    "# Install required packages\n",
    "# Ensure you have the necessary packages installed\n",
    "#this one not worked out used second approach\n",
    "%pip install sentence-transformers\n",
    "\n",
    "import chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "def store_in_chromadb(chunks):\n",
    "    \"\"\"\n",
    "    Store chunks in ChromaDB vector database\n",
    "    \"\"\"\n",
    "    print(\"Setting up ChromaDB vector store...\")\n",
    "    \n",
    "    # Create embeddings model\n",
    "    print(\"Loading embedding model...\")\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    \n",
    "    # Create vector store directory\n",
    "    vector_store_path = '../vector_store'\n",
    "    os.makedirs(vector_store_path, exist_ok=True)\n",
    "    \n",
    "    # Create ChromaDB vector store\n",
    "    print(\"Creating ChromaDB vector store...\")\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=vector_store_path,\n",
    "        collection_name=\"sj_developers_knowledge\"\n",
    "    )\n",
    "    \n",
    "    # Persist to disk\n",
    "    vectordb.persist()\n",
    "    \n",
    "    print(f\"‚úÖ ChromaDB created successfully!\")\n",
    "    print(f\"üìÅ Location: {vector_store_path}\")\n",
    "    print(f\"üìä Total vectors: {len(chunks)}\")\n",
    "    print(f\"üè∑Ô∏è  Collection: sj_developers_knowledge\")\n",
    "    \n",
    "    return vectordb\n",
    "\n",
    "# Store chunks in ChromaDB\n",
    "if 'chunks' in locals() and chunks:\n",
    "    vectordb = store_in_chromadb(chunks)\n",
    "else:\n",
    "    print(\"‚ùå No chunks available for storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating simple ChromaDB vector store...\n",
      "Found existing collection: sj_developers_knowledge\n",
      "Adding 83 documents to ChromaDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SreeChow/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79.3M/79.3M [00:35<00:00, 2.32MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added batch 1/1\n",
      "‚úÖ ChromaDB created successfully!\n",
      "üìÅ Location: ../vector_store\n",
      "üìä Total documents: 83\n",
      "\n",
      "üîç Testing search functionality...\n",
      "Found 3 results:\n",
      "\n",
      "Result 1:\n",
      "Source: SJ_Developers.pdf\n",
      "Content: SJ DevelopersAnantapuram, Andhra Pradesh., India...\n",
      "\n",
      "Result 2:\n",
      "Source: SJ_Developers.pdf\n",
      "Content: SJ Developers is a real-estate development company, which is registered at Anantapuram, Andhra Pradesh, India.What we do?vFirst, we make a development...\n",
      "\n",
      "Result 3:\n",
      "Source: CIBW1022010.pdf\n",
      "Content: trial project is a commercial building in ‚Äúopen book‚Äù, meaning the client is another company from the \n",
      "same Group Corporation. The system was presente...\n"
     ]
    }
   ],
   "source": [
    "# Simple approach using basic ChromaDB without external embeddings\n",
    "#Let's use a simpler approach without sentence-transformers that might have compatibility issues:\n",
    "\n",
    "\n",
    "import chromadb\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def create_simple_chromadb(chunks):\n",
    "    \"\"\"\n",
    "    Create ChromaDB using default embeddings (no external dependencies)\n",
    "    \"\"\"\n",
    "    print(\"Creating simple ChromaDB vector store...\")\n",
    "    \n",
    "    # Create vector store directory\n",
    "    vector_store_path = Path('../vector_store')\n",
    "    vector_store_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Initialize ChromaDB client\n",
    "    client = chromadb.PersistentClient(path=str(vector_store_path))\n",
    "    \n",
    "    # Create or get collection\n",
    "    collection_name = \"sj_developers_knowledge\"\n",
    "    \n",
    "    try:\n",
    "        # Try to get existing collection\n",
    "        collection = client.get_collection(collection_name)\n",
    "        print(f\"Found existing collection: {collection_name}\")\n",
    "    except:\n",
    "        # Create new collection\n",
    "        collection = client.create_collection(collection_name)\n",
    "        print(f\"Created new collection: {collection_name}\")\n",
    "    \n",
    "    # Prepare documents for ChromaDB\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        documents.append(chunk.page_content)\n",
    "        metadatas.append({\n",
    "            'source_file': chunk.metadata.get('source_file', 'unknown'),\n",
    "            'page': str(chunk.metadata.get('page', 0)),\n",
    "            'chunk_id': i\n",
    "        })\n",
    "        ids.append(f\"chunk_{i}\")\n",
    "    \n",
    "    # Add documents to collection\n",
    "    print(f\"Adding {len(documents)} documents to ChromaDB...\")\n",
    "    \n",
    "    # Add in batches to avoid memory issues\n",
    "    batch_size = 100\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch_docs = documents[i:i+batch_size]\n",
    "        batch_meta = metadatas[i:i+batch_size]\n",
    "        batch_ids = ids[i:i+batch_size]\n",
    "        \n",
    "        collection.add(\n",
    "            documents=batch_docs,\n",
    "            metadatas=batch_meta,\n",
    "            ids=batch_ids\n",
    "        )\n",
    "        print(f\"Added batch {i//batch_size + 1}/{(len(documents)-1)//batch_size + 1}\")\n",
    "    \n",
    "    print(f\"‚úÖ ChromaDB created successfully!\")\n",
    "    print(f\"üìÅ Location: {vector_store_path}\")\n",
    "    print(f\"üìä Total documents: {collection.count()}\")\n",
    "    \n",
    "    return client, collection\n",
    "\n",
    "# Create ChromaDB\n",
    "if 'chunks' in locals() and chunks:\n",
    "    client, collection = create_simple_chromadb(chunks)\n",
    "    \n",
    "    # Test the database\n",
    "    print(\"\\nüîç Testing search functionality...\")\n",
    "    results = collection.query(\n",
    "        query_texts=[\"What is SJ Developers?\"],\n",
    "        n_results=3\n",
    "    )\n",
    "    \n",
    "    print(f\"Found {len(results['documents'][0])} results:\")\n",
    "    for i, doc in enumerate(results['documents'][0]):\n",
    "        print(f\"\\nResult {i+1}:\")\n",
    "        print(f\"Source: {results['metadatas'][0][i]['source_file']}\")\n",
    "        print(f\"Content: {doc[:150]}...\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No chunks available. Run chunking code first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç TESTING KNOWLEDGE BASE:\n",
      "==================================================\n",
      "\n",
      "Query: 'What is SJ Developers?'\n",
      "------------------------------\n",
      "\n",
      "Result 1: SJ_Developers.pdf\n",
      "Content: SJ DevelopersAnantapuram, Andhra Pradesh., India...\n",
      "\n",
      "Result 2: SJ_Developers.pdf\n",
      "Content: SJ Developers is a real-estate development company, which is registered at Anantapuram, Andhra Pradesh, India.What we do?vFirst, we make a development agreement with the owner of the open land and tak...\n",
      "\n",
      "Query: 'Tell me about villas'\n",
      "------------------------------\n",
      "\n",
      "Result 1: VILLA 02.pdf\n",
      "Content: LIVING14'11\"X13'6\"\n",
      "BED ROOM10'1\"X10'4\"M BED ROOM11'9\"X10'4\"\n",
      "G BATH4'6\"X7'0\"\n",
      "A BATH4'0\"X8'0\"\n",
      "PARKING12'5\"X12'4\"\n",
      "3'\n",
      "2'\n",
      "2'\n",
      "D D\n",
      "ED\n",
      "D1\n",
      "D1 2'\n",
      "CLIENT:SOLLAPURAM,ANANTAPURDIST.\n",
      "LOCATION:\n",
      "ENGINEERS - DESIGNERS...\n",
      "\n",
      "Result 2: VILLA 01.pdf\n",
      "Content: UPLIVING14'8\"X12'0\"\n",
      "KITCHEN12'3\"X6'7\"\n",
      "DINING11'11\"x10'7\"\n",
      "BED ROOM9'11\"X9'9\"M BED ROOM11'11\"X9'9\"\n",
      "G BATH4'0\"X6'5\"\n",
      "A BATH4'0\"X6'0\"\n",
      "PARKING13'3\"x13'5\"3'6\" WIDEUTILITY\n",
      "POOJA3'6\"X4'0\"\n",
      "3'\n",
      "2'-6\"\n",
      "2'-6\"\n",
      "T V UN...\n",
      "\n",
      "Query: 'Registration information'\n",
      "------------------------------\n",
      "\n",
      "Result 1: CIBW1022010.pdf\n",
      "Content: accomplished by: i) providing all the information needed throughout every department and \n",
      "construction site in the company; ii) creating an intranet portal, defined as a central Web site for \n",
      "organizi...\n",
      "\n",
      "Result 2: CIBW1022010.pdf\n",
      "Content: system developed by Dawood et al. (2002) also uses internet technology and it is formed by three key \n",
      "elements: Web Server, Client Server and User Interface. The link between the two servers is provid...\n",
      "\n",
      "Query: 'Amari Farms details'\n",
      "------------------------------\n",
      "\n",
      "Result 1: AmariFarms_Brochure.pdf\n",
      "Content: LEGEND \n",
      "CATEGORY\n",
      "1        Acre \n",
      "0.5     Acre \n",
      "0.25   Acre \n",
      "Odd   Plots \n",
      "S\n",
      "N\n",
      "W\n",
      "E\n",
      "C\n",
      "E|C\n",
      "W|C\n",
      "N|C\n",
      "- South Facing\n",
      "- Noth Facing\n",
      "- West Facing\n",
      "- East Facing\n",
      " (PLC Applicable)\n",
      "- Corner Facing (PLC Applicable...\n",
      "\n",
      "Result 2: SJ_Developers.pdf\n",
      "Content: Previous projects: ‚Ä¢We did partnership with other companies and gained a good experience in real-estate business‚Ä¢Two of the projects we have worked are with-in proximity of Anantapuram cityCurrent Pro...\n"
     ]
    }
   ],
   "source": [
    "#this is a function to search the knowledge base\n",
    "def search_knowledge_base(client, collection_name, query, n_results=3):\n",
    "    \"\"\"\n",
    "    Search the knowledge base\n",
    "    \"\"\"\n",
    "    collection = client.get_collection(collection_name)\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test different queries\n",
    "if 'client' in locals() and 'collection' in locals():\n",
    "    test_queries = [\n",
    "        \"What is SJ Developers?\",\n",
    "        \"Tell me about villas\",\n",
    "        \"Registration information\",\n",
    "        \"Amari Farms details\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüîç TESTING KNOWLEDGE BASE:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\nQuery: '{query}'\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        results = search_knowledge_base(client, \"sj_developers_knowledge\", query, 2)\n",
    "        \n",
    "        for i, doc in enumerate(results['documents'][0]):\n",
    "            source = results['metadatas'][0][i]['source_file']\n",
    "            print(f\"\\nResult {i+1}: {source}\")\n",
    "            print(f\"Content: {doc[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Gradio interface...\n",
      "‚úÖ Knowledge base loaded: 83 documents\n",
      "Launching knowledge worker interface...\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Knowledge base loaded: 83 documents\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import chromadb\n",
    "from pathlib import Path\n",
    "\n",
    "def load_knowledge_base():\n",
    "    \"\"\"\n",
    "    Load the existing ChromaDB knowledge base\n",
    "    \"\"\"\n",
    "    try:\n",
    "        vector_store_path = Path('../vector_store')\n",
    "        client = chromadb.PersistentClient(path=str(vector_store_path))\n",
    "        collection = client.get_collection(\"sj_developers_knowledge\")\n",
    "        print(f\"‚úÖ Knowledge base loaded: {collection.count()} documents\")\n",
    "        return client, collection\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading knowledge base: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def search_documents(query, num_results=3):\n",
    "    \"\"\"\n",
    "    Search the knowledge base and return formatted results\n",
    "    \"\"\"\n",
    "    if not query.strip():\n",
    "        return \"Please enter a question to search the knowledge base.\"\n",
    "    \n",
    "    try:\n",
    "        # Load knowledge base\n",
    "        client, collection = load_knowledge_base()\n",
    "        \n",
    "        if not collection:\n",
    "            return \"‚ùå Knowledge base not found. Please create the vector database first.\"\n",
    "        \n",
    "        # Search the collection\n",
    "        results = collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=num_results\n",
    "        )\n",
    "        \n",
    "        if not results['documents'][0]:\n",
    "            return \"No relevant documents found for your query.\"\n",
    "        \n",
    "        # Format results\n",
    "        response = f\"üîç **Search Results for:** *{query}*\\n\\n\"\n",
    "        response += f\"üìä **Found {len(results['documents'][0])} relevant documents:**\\n\\n\"\n",
    "        \n",
    "        for i, doc in enumerate(results['documents'][0]):\n",
    "            source = results['metadatas'][0][i]['source_file']\n",
    "            page = results['metadatas'][0][i].get('page', 'N/A')\n",
    "            \n",
    "            response += f\"### üìÑ Result {i+1}: {source}\\n\"\n",
    "            response += f\"**Page:** {page}\\n\\n\"\n",
    "            response += f\"**Content:**\\n{doc[:400]}...\\n\\n\"\n",
    "            response += \"---\\n\\n\"\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error searching knowledge base: {str(e)}\"\n",
    "\n",
    "def get_knowledge_stats():\n",
    "    \"\"\"\n",
    "    Get statistics about the knowledge base\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client, collection = load_knowledge_base()\n",
    "        if collection:\n",
    "            count = collection.count()\n",
    "            return f\"üìä Knowledge Base contains {count} document chunks\"\n",
    "        else:\n",
    "            return \"‚ùå Knowledge base not available\"\n",
    "    except:\n",
    "        return \"‚ùå Error accessing knowledge base\"\n",
    "\n",
    "# Create Gradio Interface\n",
    "def create_gradio_interface():\n",
    "    \"\"\"\n",
    "    Create the Gradio web interface\n",
    "    \"\"\"\n",
    "    with gr.Blocks(title=\"SJ Developers Knowledge Worker\", theme=gr.themes.Soft()) as app:\n",
    "        \n",
    "        # Header\n",
    "        gr.Markdown(\"\"\"\n",
    "        # üè¢ SJ Developers Knowledge Worker\n",
    "        ### Ask questions about your PDF documents\n",
    "        \"\"\")\n",
    "        \n",
    "        # Knowledge base stats\n",
    "        stats = get_knowledge_stats()\n",
    "        gr.Markdown(f\"**Status:** {stats}\")\n",
    "        \n",
    "        # Main interface\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=2):\n",
    "                # Input section\n",
    "                query_input = gr.Textbox(\n",
    "                    label=\"üîç Ask a question\",\n",
    "                    placeholder=\"e.g., What is SJ Developers? Tell me about villas...\",\n",
    "                    lines=2\n",
    "                )\n",
    "                \n",
    "                num_results = gr.Slider(\n",
    "                    minimum=1,\n",
    "                    maximum=5,\n",
    "                    value=3,\n",
    "                    step=1,\n",
    "                    label=\"Number of results\"\n",
    "                )\n",
    "                \n",
    "                search_btn = gr.Button(\"üîç Search Knowledge Base\", variant=\"primary\")\n",
    "                clear_btn = gr.Button(\"üßπ Clear\", variant=\"secondary\")\n",
    "        \n",
    "        # Output section\n",
    "        with gr.Row():\n",
    "            output = gr.Markdown(\n",
    "                label=\"üìã Search Results\",\n",
    "                value=\"Enter a question above to search the knowledge base.\"\n",
    "            )\n",
    "        \n",
    "        # Example queries\n",
    "        gr.Markdown(\"\"\"\n",
    "        ### üí° Example Questions:\n",
    "        - What is SJ Developers?\n",
    "        - Tell me about the available villas\n",
    "        - What are the registration details?\n",
    "        - Information about Amari Farms\n",
    "        - What properties are available?\n",
    "        \"\"\")\n",
    "        \n",
    "        # Event handlers\n",
    "        search_btn.click(\n",
    "            fn=search_documents,\n",
    "            inputs=[query_input, num_results],\n",
    "            outputs=output\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            fn=lambda: (\"\", \"Enter a question above to search the knowledge base.\"),\n",
    "            outputs=[query_input, output]\n",
    "        )\n",
    "        \n",
    "        # Allow Enter key to trigger search\n",
    "        query_input.submit(\n",
    "            fn=search_documents,\n",
    "            inputs=[query_input, num_results],\n",
    "            outputs=output\n",
    "        )\n",
    "    \n",
    "    return app\n",
    "\n",
    "# Launch the interface\n",
    "print(\"Creating Gradio interface...\")\n",
    "app = create_gradio_interface()\n",
    "\n",
    "# Launch with public sharing (optional)\n",
    "print(\"Launching knowledge worker interface...\")\n",
    "app.launch(\n",
    "    share=False,  # Set to True if you want a public link\n",
    "    server_name=\"127.0.0.1\",\n",
    "    server_port=7860,\n",
    "    show_error=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating chat interface...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x5/gxfv5zh57dd8qw9f49g4t1rh0000gn/T/ipykernel_9826/3623651953.py:55: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enhanced version with chat-like interface\n",
    "import gradio as gr\n",
    "import chromadb\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def create_chat_interface():\n",
    "    \"\"\"\n",
    "    Create a chat-like interface for the knowledge base\n",
    "    \"\"\"\n",
    "    def chat_with_knowledge_base(message, history):\n",
    "        \"\"\"\n",
    "        Process chat message and return response with history\n",
    "        \"\"\"\n",
    "        if not message.strip():\n",
    "            return history, \"\"\n",
    "        \n",
    "        try:\n",
    "            # Load knowledge base\n",
    "            vector_store_path = Path('../vector_store')\n",
    "            client = chromadb.PersistentClient(path=str(vector_store_path))\n",
    "            collection = client.get_collection(\"sj_developers_knowledge\")\n",
    "            \n",
    "            # Search for relevant documents\n",
    "            results = collection.query(\n",
    "                query_texts=[message],\n",
    "                n_results=2\n",
    "            )\n",
    "            \n",
    "            if results['documents'][0]:\n",
    "                # Format response\n",
    "                response = \"Based on your documents:\\n\\n\"\n",
    "                \n",
    "                for i, doc in enumerate(results['documents'][0]):\n",
    "                    source = results['metadatas'][0][i]['source_file']\n",
    "                    response += f\"üìÑ **From {source}:**\\n\"\n",
    "                    response += f\"{doc[:300]}...\\n\\n\"\n",
    "                \n",
    "                # Add to history\n",
    "                history.append([message, response])\n",
    "            else:\n",
    "                response = \"I couldn't find relevant information in your documents for that question.\"\n",
    "                history.append([message, response])\n",
    "                \n",
    "        except Exception as e:\n",
    "            response = f\"Error searching knowledge base: {str(e)}\"\n",
    "            history.append([message, response])\n",
    "        \n",
    "        return history, \"\"\n",
    "    \n",
    "    # Create chat interface\n",
    "    with gr.Blocks(title=\"SJ Developers Chat\", theme=gr.themes.Soft()) as chat_app:\n",
    "        gr.Markdown(\"# üí¨ Chat with SJ Developers Knowledge Base\")\n",
    "        \n",
    "        chatbot = gr.Chatbot(\n",
    "            label=\"Knowledge Worker Assistant\",\n",
    "            height=400,\n",
    "            show_label=True\n",
    "        )\n",
    "        \n",
    "        with gr.Row():\n",
    "            msg = gr.Textbox(\n",
    "                label=\"Message\",\n",
    "                placeholder=\"Ask me anything about your documents...\",\n",
    "                scale=4\n",
    "            )\n",
    "            submit = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
    "        \n",
    "        clear = gr.Button(\"Clear Chat\")\n",
    "        \n",
    "        # Event handlers\n",
    "        submit.click(\n",
    "            chat_with_knowledge_base,\n",
    "            inputs=[msg, chatbot],\n",
    "            outputs=[chatbot, msg]\n",
    "        )\n",
    "        \n",
    "        msg.submit(\n",
    "            chat_with_knowledge_base,\n",
    "            inputs=[msg, chatbot],\n",
    "            outputs=[chatbot, msg]\n",
    "        )\n",
    "        \n",
    "        clear.click(lambda: [], outputs=chatbot)\n",
    "    \n",
    "    return chat_app\n",
    "\n",
    "# Launch chat interface\n",
    "print(\"Creating chat interface...\")\n",
    "chat_app = create_chat_interface()\n",
    "chat_app.launch(share=False, server_port=7861)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
